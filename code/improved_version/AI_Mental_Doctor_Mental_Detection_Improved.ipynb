{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f27d5fd4",
   "metadata": {},
   "source": [
    "Cell 1 — Upload / prepare data (run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9252c992",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 1 — Upload / Prepare Data (Run once)\n",
    "# ============================================\n",
    "# Purpose:\n",
    "# - Manage dataset uploads and preparation using an object-oriented approach.\n",
    "# - Robust handling with try/except for errors during file operations.\n",
    "#\n",
    "# Expected files:\n",
    "#   1) training.1600000.processed.noemoticon.csv\n",
    "#   2) depressive_tweets_processed.csv\n",
    "#   3) contractions.json\n",
    "#\n",
    "# Notes:\n",
    "# - Safe to re-run; existing files are preserved unless replaced intentionally.\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "class DataManager:\n",
    "    \"\"\"\n",
    "    Class to manage dataset uploads and preparation in Google Colab.\n",
    "    Includes error handling for robust execution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir=\"/content/project_data\"):\n",
    "        self.data_dir = data_dir\n",
    "        try:\n",
    "            os.makedirs(self.data_dir, exist_ok=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating data directory {self.data_dir}: {e}\")\n",
    "\n",
    "    def list_files(self):\n",
    "        \"\"\"Return a list of files currently in the data directory.\"\"\"\n",
    "        try:\n",
    "            return os.listdir(self.data_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"Error listing files in {self.data_dir}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def display_status(self):\n",
    "        \"\"\"Display current files in the data directory.\"\"\"\n",
    "        files = self.list_files()\n",
    "        print(f\"Data directory: {self.data_dir}\")\n",
    "        if files:\n",
    "            print(\"Existing files:\")\n",
    "            for f in files:\n",
    "                print(f\" - {f}\")\n",
    "        else:\n",
    "            print(\"No files currently in the data directory.\")\n",
    "\n",
    "    def upload_files(self):\n",
    "        \"\"\"Launch file upload dialog and move uploaded files to the data directory.\"\"\"\n",
    "        print(\"Please select the required files to upload.\")\n",
    "        try:\n",
    "            uploaded = files.upload()\n",
    "            for fname in uploaded.keys():\n",
    "                try:\n",
    "                    dest_path = os.path.join(self.data_dir, fname)\n",
    "                    if os.path.exists(dest_path):\n",
    "                        print(f\"Replacing existing file: {fname}\")\n",
    "                        os.remove(dest_path)\n",
    "                    shutil.move(fname, dest_path)\n",
    "                except Exception as file_error:\n",
    "                    print(f\"Error moving file {fname}: {file_error}\")\n",
    "            print(\"Upload complete.\")\n",
    "        except Exception as upload_error:\n",
    "            print(f\"Error during file upload: {upload_error}\")\n",
    "\n",
    "    def prepare(self):\n",
    "        \"\"\"\n",
    "        Main workflow:\n",
    "        1. Display current files.\n",
    "        2. Ask user whether to upload/replace files.\n",
    "        3. Show final directory contents.\n",
    "        \"\"\"\n",
    "        self.display_status()\n",
    "        try:\n",
    "            user_choice = input(\"\\nDo you want to upload or replace files? (y/n): \").strip().lower()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading user input: {e}\")\n",
    "            user_choice = 'n'\n",
    "\n",
    "        if user_choice == 'y':\n",
    "            self.upload_files()\n",
    "\n",
    "        print(\"\\nFinal files in data directory:\")\n",
    "        for f in self.list_files():\n",
    "            print(f\" - {f}\")\n",
    "        print(\"\\nData preparation complete.\")\n",
    "\n",
    "\n",
    "# --- Execute the workflow ---\n",
    "if __name__ == \"__main__\":\n",
    "    data_manager = DataManager()\n",
    "    data_manager.prepare()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe623ed",
   "metadata": {},
   "source": [
    "Cell 2 — Install & Imports (environment + GloVe helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f933fb22",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cell 2 — Install & Imports (environment + GloVe helper)\n",
    "# ============================\n",
    "# Purpose:\n",
    "# - Install small helper packages if missing (kept conservative to avoid binary mismatch).\n",
    "# - Import all libraries used across the project.\n",
    "# - Provide helper to download GloVe 100d (optional). If not available, later cells will fall back to random embeddings.\n",
    "# - Use try/except import pattern so the runtime is robust\n",
    "\n",
    "# Install light helper packages quietly. If don't want to install, comment the line.\n",
    "\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class EnvironmentSetup:\n",
    "    \"\"\"\n",
    "    Handles package installation, imports, reproducibility, and GloVe embedding checks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seed: int = 42, glove_dir: str = \"/content/glove\"):\n",
    "        self.seed = seed\n",
    "        self.glove_dir = glove_dir\n",
    "        self.glove_filename = \"glove.6B.100d.txt\"\n",
    "        self.glove_zip = \"/content/glove.6B.zip\"\n",
    "        self.glove_path = os.path.join(self.glove_dir, self.glove_filename)\n",
    "\n",
    "        # Logging setup\n",
    "        logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "        self.logger = logging.getLogger(\"DepressionProject\")\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "\n",
    "    def set_seed(self):\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(self.seed)\n",
    "        self.logger.info(\"Random seed set to %d\", self.seed)\n",
    "\n",
    "    def install_packages(self):\n",
    "        \"\"\"Install light helper packages if missing.\"\"\"\n",
    "        try:\n",
    "            import preprocessor as _p_test\n",
    "            from wordcloud import WordCloud as _wc_test\n",
    "        except Exception:\n",
    "            !pip install -q tweet-preprocessor wordcloud tqdm\n",
    "\n",
    "        try:\n",
    "            import nltk\n",
    "        except Exception:\n",
    "            !pip install -q nltk\n",
    "\n",
    "        try:\n",
    "            import tensorflow as tf\n",
    "        except Exception:\n",
    "            !pip install -q tensorflow\n",
    "\n",
    "    def import_packages(self):\n",
    "        \"\"\"Import all necessary packages after installation.\"\"\"\n",
    "        import re\n",
    "        import time\n",
    "        import json\n",
    "        import pickle\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from tqdm import tqdm\n",
    "        import nltk\n",
    "        import preprocessor as p\n",
    "        from nltk.corpus import stopwords\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        from wordcloud import WordCloud\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "        from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "        from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, GlobalMaxPool1D, Dense, Dropout, Layer\n",
    "        from tensorflow.keras.models import Model, load_model\n",
    "        from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "        from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        from sklearn.naive_bayes import MultinomialNB\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.svm import LinearSVC\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "        import joblib\n",
    "        self.logger.info(\"All packages imported successfully.\")\n",
    "\n",
    "    def download_nltk_resources(self):\n",
    "        \"\"\"Download required NLTK resources.\"\"\"\n",
    "        import nltk\n",
    "        try:\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            nltk.download('punkt_tab', quiet=True)\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            self.logger.info(\"NLTK resources downloaded successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.warning(\"NLTK download issue: %s\", e)\n",
    "\n",
    "    def ensure_glove_100d(self, download_if_missing: bool = False) -> bool:\n",
    "        \"\"\"Check if GloVe embeddings exist, optionally download them.\"\"\"\n",
    "        if os.path.exists(self.glove_path):\n",
    "            self.logger.info(\"GloVe found at %s\", self.glove_path)\n",
    "            return True\n",
    "        if not download_if_missing:\n",
    "            self.logger.info(\"GloVe not found at %s. To download, call ensure_glove_100d(download_if_missing=True)\", self.glove_path)\n",
    "            return False\n",
    "        try:\n",
    "            self.logger.info(\"Downloading GloVe (large file, patience required)...\")\n",
    "            !wget -q -nc http://nlp.stanford.edu/data/glove.6B.zip -P /content\n",
    "            os.makedirs(self.glove_dir, exist_ok=True)\n",
    "            !unzip -q -o /content/glove.6B.zip -d /content/glove\n",
    "            found = os.path.exists(self.glove_path)\n",
    "            if found:\n",
    "                self.logger.info(\"GloVe extracted to %s\", self.glove_path)\n",
    "            else:\n",
    "                self.logger.warning(\"GloVe 100d not found after extraction.\")\n",
    "            return found\n",
    "        except Exception as e:\n",
    "            self.logger.warning(\"Failed to download/extract GloVe: %s\", e)\n",
    "            return False\n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"Run full environment setup.\"\"\"\n",
    "        self.set_seed()\n",
    "        self.install_packages()\n",
    "        self.import_packages()\n",
    "        self.download_nltk_resources()\n",
    "        glove_available = self.ensure_glove_100d(download_if_missing=False)\n",
    "        self.logger.info(\"GloVe 100d available: %s\", glove_available)\n",
    "        self.logger.info(\"Environment setup complete.\")\n",
    "\n",
    "\n",
    "# --- Execute environment setup ---\n",
    "if __name__ == \"__main__\":\n",
    "    env_setup = EnvironmentSetup()\n",
    "    env_setup.setup()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269248dc",
   "metadata": {},
   "source": [
    "Cell 3 — Configuration Manager — hyperparameters editable at runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df696fb3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cell 3 — Configuration Manager\n",
    "# ============================\n",
    "# Purpose:\n",
    "# - Provide a small wrapper for experiment configuration so you can adjust hyperparameters\n",
    "#   interactively from one place during testing.\n",
    "# - Keeps the code clean and consistent.\n",
    "# - Supports serialization to JSON for experiment tracking.\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# --- Configuration Dataclass ---\n",
    "@dataclass\n",
    "class Config:\n",
    "    # --- File paths and directories ---\n",
    "    data_dir: str = \"/content/project_data\"\n",
    "    data_path: str = \"/content/project_data\"\n",
    "    glove_path: str = \"/content/glove.6B.100d.txt\"\n",
    "\n",
    "    sentiment_file: str = \"training.1600000.processed.noemoticon.csv\"\n",
    "    depressive_file: str = \"depressive_tweets_processed.csv\"\n",
    "    contractions_file: str = \"contractions.json\"\n",
    "\n",
    "    cache_dir: str = \"/content/cache\"\n",
    "    save_dir: str = \"/content/drive/MyDrive/project_results\"\n",
    "\n",
    "    # --- Data sampling ---\n",
    "    sentiment_sample: int = 18000\n",
    "    depressive_rows: int = 9000\n",
    "\n",
    "    # --- Deep Model Parameters ---\n",
    "    max_num_words: int = 30000\n",
    "    max_seq_length: int = 200\n",
    "    embedding_dim: int = 100\n",
    "    lstm_units: int = 128\n",
    "    lstm_layers: int = 2\n",
    "    dropout_rate: float = 0.4\n",
    "    recurrent_dropout: float = 0.3\n",
    "    dense_units: int = 128\n",
    "    learning_rate: float = 2e-4\n",
    "    deep_batch_size: int = 64\n",
    "    deep_epochs: int = 10\n",
    "    patience: int = 3\n",
    "\n",
    "    # --- Classical Model ---\n",
    "    classical_cv_folds: int = 3\n",
    "    grid_search_n_jobs: int = -1\n",
    "\n",
    "    # --- Experiment Control ---\n",
    "    seed: int = 42\n",
    "    fast_mode: bool = False\n",
    "    log_to_drive: bool = True\n",
    "\n",
    "    # --- Utility: Convert to dict for saving/logging ---\n",
    "    def to_dict(self):\n",
    "        \"\"\"Return configuration parameters as a dictionary for logging or saving.\"\"\"\n",
    "        return asdict(self)\n",
    "\n",
    "# --- Create directories safely ---\n",
    "os.makedirs(Config().cache_dir, exist_ok=True)\n",
    "os.makedirs(Config().save_dir, exist_ok=True)\n",
    "\n",
    "# --- Logging utility ---\n",
    "def setup_logger():\n",
    "    logger = logging.getLogger(\"DepressionProject\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    if not logger.handlers:\n",
    "        ch = logging.StreamHandler()\n",
    "        formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "        ch.setFormatter(formatter)\n",
    "        logger.addHandler(ch)\n",
    "    return logger\n",
    "\n",
    "logger = setup_logger()\n",
    "\n",
    "# --- Logging experiment metadata ---\n",
    "def log_experiment(cfg: Config):\n",
    "    meta = {\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d_%H-%M-%S\"),\n",
    "        \"model\": \"BiLSTM + Attention\",\n",
    "        \"max_words\": cfg.max_num_words,\n",
    "        \"seq_len\": cfg.max_seq_length,\n",
    "        \"embedding_dim\": cfg.embedding_dim,\n",
    "        \"lstm_units\": cfg.lstm_units,\n",
    "        \"lstm_layers\": cfg.lstm_layers,\n",
    "        \"dropout\": cfg.dropout_rate,\n",
    "        \"recurrent_dropout\": cfg.recurrent_dropout,\n",
    "        \"learning_rate\": cfg.learning_rate,\n",
    "        \"batch_size\": cfg.deep_batch_size,\n",
    "        \"epochs\": cfg.deep_epochs\n",
    "    }\n",
    "    log_path = os.path.join(cfg.save_dir, f\"experiment_meta_{meta['timestamp']}.txt\")\n",
    "    with open(log_path, \"w\") as f:\n",
    "        for k, v in meta.items():\n",
    "            f.write(f\"{k}: {v}\\n\")\n",
    "    logger.info(f\"✅ Experiment metadata logged to: {log_path}\")\n",
    "\n",
    "# --- Instantiate and log ---\n",
    "config = Config()\n",
    "log_experiment(config)\n",
    "\n",
    "logger.info(\"✅ Configuration ready. Deep model optimized for improved accuracy and generalization.\")\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def now_str() -> str:\n",
    "    \"\"\"Return current timestamp string for saving results.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eb830b",
   "metadata": {},
   "source": [
    "Cell 4 — Text Cleaning & Preprocessing (class with caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c09701",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cell 4 — Text Cleaning & Preprocessing (with caching)\n",
    "# ============================\n",
    "# Purpose:\n",
    "# - Encapsulate tweet cleaning steps in a reusable class.\n",
    "# - Provide caching to skip repeated cleaning across runs (speeds iterative development).\n",
    "# - Steps implemented:\n",
    "#     * Lowercasing\n",
    "#     * tweet-preprocessor cleaning (removes urls/mentions/hashtags normalizations)\n",
    "#     * Contraction expansion (if contractions.json provided)\n",
    "#     * Remove non-alphanumeric symbols (keeps # + _ as in earlier code)\n",
    "#     * Tokenize and remove NLTK stopwords\n",
    "# - Use tqdm progress bar for long cleaning loops.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"DepressionProject\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "class DataLoaderLocal:\n",
    "    \"\"\"\n",
    "    Helper to load CSV files from the configured data_dir.\n",
    "    Uses the ExperimentConfig object for paths and sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: Config):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def _path(self, fname: str) -> str:\n",
    "        \"\"\"Return full path for a filename in data_dir.\"\"\"\n",
    "        return os.path.join(self.cfg.data_dir, fname)\n",
    "\n",
    "    def load_sentiment140(self, sample_n: Optional[int] = None, encoding: str = \"ISO-8859-1\") -> pd.DataFrame:\n",
    "        \"\"\"Load a sample of the Sentiment140 dataset, assign label=0.\"\"\"\n",
    "        sample_n = sample_n or self.cfg.sentiment_sample\n",
    "        path = self._path(self.cfg.sentiment_file)\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Sentiment140 not found at {path}\")\n",
    "        col_names = ['target','id','date','flag','user','text']\n",
    "        df = pd.read_csv(path, names=col_names, encoding=encoding)\n",
    "        df_sample = df[['text']].sample(sample_n, random_state=self.cfg.seed).reset_index(drop=True)\n",
    "        df_sample['label'] = 0\n",
    "        return df_sample\n",
    "\n",
    "    def load_depressive(self, nrows: Optional[int] = None) -> pd.DataFrame:\n",
    "        \"\"\"Load depressive tweets dataset, assign label=1.\"\"\"\n",
    "        nrows = nrows or self.cfg.depressive_rows\n",
    "        path = self._path(self.cfg.depressive_file)\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Depressive tweets file not found at {path}\")\n",
    "        df = pd.read_csv(path, sep='|', header=None, usecols=[0,5], nrows=nrows, names=['id','text'])\n",
    "        df = df[['text']].reset_index(drop=True)\n",
    "        df['label'] = 1\n",
    "        return df\n",
    "\n",
    "    def load_contractions(self) -> Dict[str,str]:\n",
    "        \"\"\"Load contraction mapping from JSON file; return lowercase-normalized keys.\"\"\"\n",
    "        path = self._path(self.cfg.contractions_file)\n",
    "        if not os.path.exists(path):\n",
    "            logger.warning(\"Contractions file not found at %s. Continuing without contractions.\", path)\n",
    "            return {}\n",
    "        s = pd.read_json(path, typ='series')\n",
    "        return {str(k).lower(): v for k, v in s.to_dict().items()}\n",
    "\n",
    "\n",
    "class PreprocessorWithCache:\n",
    "    \"\"\"\n",
    "    Text preprocessor with optional caching.\n",
    "    Cleans text with lowercasing, tweet-preprocessor, contraction expansion,\n",
    "    symbol removal, and stopword removal.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, contractions: Optional[Dict[str,str]] = None, cache_dir: Optional[str] = None):\n",
    "        self.contractions = contractions or {}\n",
    "        # Compile contraction regex if available\n",
    "        if self.contractions:\n",
    "            keys = sorted(self.contractions.keys(), key=lambda x: -len(x))\n",
    "            pattern = '|'.join(map(re.escape, keys))\n",
    "            self.c_re = re.compile(r'(%s)' % pattern)\n",
    "        else:\n",
    "            self.c_re = None\n",
    "\n",
    "        self.BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.cache_dir = cache_dir or config.cache_dir\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "\n",
    "    def expand_contractions(self, text: str) -> str:\n",
    "        \"\"\"Replace contractions in text using the provided mapping.\"\"\"\n",
    "        if not self.c_re:\n",
    "            return text\n",
    "        return self.c_re.sub(lambda m: self.contractions.get(m.group(0), m.group(0)), text)\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean a single text/tweet:\n",
    "        - Lowercase\n",
    "        - Remove URLs/mentions/emoji via tweet-preprocessor\n",
    "        - Expand contractions\n",
    "        - Remove unwanted symbols (keep letters, digits, # + _)\n",
    "        - Tokenize and remove stopwords\n",
    "        Returns cleaned string (tokens joined by spaces).\n",
    "        \"\"\"\n",
    "        t = str(text).lower()\n",
    "        t = p.clean(t)  # tweet-preprocessor\n",
    "        t = self.expand_contractions(t)\n",
    "        t = self.BAD_SYMBOLS_RE.sub(' ', t)\n",
    "        toks = word_tokenize(t)\n",
    "        toks = [tok for tok in toks if tok not in self.stop_words]\n",
    "        return ' '.join(toks)\n",
    "\n",
    "    def _cache_file_for_key(self, key: str) -> str:\n",
    "        \"\"\"Generate a safe cache filename from a key.\"\"\"\n",
    "        safe = re.sub(r'[^0-9a-zA-Z_\\-]', '_', key)\n",
    "        return os.path.join(self.cache_dir, f\"{safe}.pkl\")\n",
    "\n",
    "    def clean_texts(self, texts: List[str], cache_key: Optional[str] = None) -> List[str]:\n",
    "        \"\"\"\n",
    "        Clean a list of texts. Uses cache if cache_key provided and cache exists.\n",
    "        Returns a list of cleaned strings.\n",
    "        \"\"\"\n",
    "        if cache_key:\n",
    "            cache_file = self._cache_file_for_key(cache_key)\n",
    "            if os.path.exists(cache_file):\n",
    "                logger.info(\"Loading cleaned texts from cache: %s\", cache_file)\n",
    "                try:\n",
    "                    return pd.read_pickle(cache_file).tolist()\n",
    "                except Exception as e:\n",
    "                    logger.warning(\"Failed to load cache, recomputing: %s\", e)\n",
    "\n",
    "        cleaned = []\n",
    "        for t in tqdm(texts, desc=\"Cleaning texts\"):\n",
    "            cleaned.append(self.clean_text(t))\n",
    "\n",
    "        if cache_key:\n",
    "            try:\n",
    "                pd.Series(cleaned).to_pickle(self._cache_file_for_key(cache_key))\n",
    "                logger.info(\"Saved cleaned texts cache to %s\", self._cache_file_for_key(cache_key))\n",
    "            except Exception as e:\n",
    "                logger.warning(\"Failed to save cache: %s\", e)\n",
    "\n",
    "        return cleaned\n",
    "\n",
    "# Example short sanity check:\n",
    "# data_loader = DataLoaderLocal(config)\n",
    "# small_sent = data_loader.load_sentiment140(sample_n=100)\n",
    "# print(\"Loaded sample shape:\", small_sent.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2773d723",
   "metadata": {},
   "source": [
    "Cell 5 — Exploratory Data Analysis (EDA) & Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ce64e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cell 5 — Exploratory Data Analysis (EDA) & Visualizer\n",
    "# ============================\n",
    "# Purpose:\n",
    "# - Provide small EDA utilities for quick dataset checks and visualizations:\n",
    "#   class distribution, sample tweets, wordclouds, top TF-IDF terms (if TF-IDF built later).\n",
    "# - These functions use the cleaned 'clean' column once you run the cleaning step in later cells.\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "class EDAVisualizer:\n",
    "    \"\"\"\n",
    "    Small visualizer class to inspect dataset and cleaned text.\n",
    "    It expects a DataFrame with columns: ['text', 'label'] or ['text', 'label', 'clean'].\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "\n",
    "    def sample_rows(self, n: int = 5, label: Optional[int] = None):\n",
    "        \"\"\"Print sample original and cleaned rows for quick inspection.\"\"\"\n",
    "        if label is None:\n",
    "            sample = self.df.sample(min(n, len(self.df)), random_state=SEED)\n",
    "        else:\n",
    "            sample = self.df[self.df['label'] == label].sample(min(n, self.df[self.df['label'] == label].shape[0]), random_state=SEED)\n",
    "        display(sample.head(n))\n",
    "\n",
    "    def class_distribution(self, savepath: Optional[str] = None):\n",
    "        \"\"\"Plot class distribution bar chart and return counts.\"\"\"\n",
    "        counts = self.df['label'].value_counts().sort_index()\n",
    "        labels = [f\"{i}\" for i in counts.index.tolist()]\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.barplot(x=labels, y=counts.values, palette=\"Blues_d\")\n",
    "        plt.title(\"Class distribution\")\n",
    "        plt.xlabel(\"Label\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        if savepath:\n",
    "            plt.savefig(savepath, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        return counts.to_dict()\n",
    "\n",
    "    def wordcloud_for_label(self, label: int = 1, max_words: int = 200, savepath: Optional[str] = None):\n",
    "        \"\"\"Generate a wordcloud for the cleaned text of a given label.\"\"\"\n",
    "        if 'clean' not in self.df.columns:\n",
    "            raise RuntimeError(\"DataFrame must contain 'clean' column (run cleaning step first).\")\n",
    "        texts = self.df[self.df['label'] == label]['clean'].dropna().astype(str).tolist()\n",
    "        if not texts:\n",
    "            print(\"No texts for label\", label)\n",
    "            return\n",
    "        words = \" \".join(texts)\n",
    "        wc = WordCloud(width=800, height=400, collocations=False, background_color='white', max_words=max_words).generate(words)\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"WordCloud for label={label}\")\n",
    "        if savepath:\n",
    "            plt.savefig(savepath, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def top_n_tokens(self, label: Optional[int] = None, n: int = 30) -> List[Tuple[str,int]]:\n",
    "        \"\"\"Return top-n token counts from cleaned text (optionally for a specific label).\"\"\"\n",
    "        if 'clean' not in self.df.columns:\n",
    "            raise RuntimeError(\"DataFrame must contain 'clean' column (run cleaning step first).\")\n",
    "        if label is None:\n",
    "            texts = self.df['clean'].dropna().astype(str).tolist()\n",
    "        else:\n",
    "            texts = self.df[self.df['label'] == label]['clean'].dropna().astype(str).tolist()\n",
    "        counter = Counter()\n",
    "        for t in texts:\n",
    "            counter.update(t.split())\n",
    "        return counter.most_common(n)\n",
    "\n",
    "# Example usage template (run after you have df and cleaned column):\n",
    "# eda = EDAVisualizer(df)\n",
    "# eda.class_distribution()\n",
    "# eda.sample_rows(n=5, label=1)\n",
    "# eda.wordcloud_for_label(label=1)\n",
    "# print(eda.top_n_tokens(label=1, n=20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240487f4",
   "metadata": {},
   "source": [
    "Cell 6 — Feature extraction & EmbeddingManager (GloVe optional, random fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394b0d19",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 6 — FeatureExtractor & EmbeddingManager (GloVe auto-download)\n",
    "# ============================================\n",
    "# ============================\n",
    "# Purpose:\n",
    "# - Build TF-IDF vectorizer for classical models.\n",
    "# - Build Keras Tokenizer + padded sequences for deep models.\n",
    "# - Build embedding matrix aligned to tokenizer.word_index:\n",
    "#     * Use GloVe if available\n",
    "#     * Otherwise produce small random Gaussian embeddings (default)\n",
    "from typing import List, Dict, Optional, Any\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from typing import Any\n",
    "import requests, zipfile\n",
    "\n",
    "# Utility to ensure GloVe embeddings exist\n",
    "def ensure_glove_100d(download_if_missing=True) -> str:\n",
    "    \"\"\"\n",
    "    Ensure GloVe 100d embeddings exist locally. Downloads if missing.\n",
    "    Returns the path to glove.6B.100d.txt\n",
    "    \"\"\"\n",
    "    glove_dir = \"/content/glove\"\n",
    "    glove_file = \"glove.6B.100d.txt\"\n",
    "    os.makedirs(glove_dir, exist_ok=True)\n",
    "    glove_path = os.path.join(glove_dir, glove_file)\n",
    "\n",
    "    if os.path.exists(glove_path):\n",
    "        logger.info(\"GloVe 100d already exists at %s\", glove_path)\n",
    "        return glove_path\n",
    "\n",
    "    if download_if_missing:\n",
    "        logger.info(\"Downloading GloVe embeddings...\")\n",
    "        url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "        zip_path = os.path.join(glove_dir, \"glove.6B.zip\")\n",
    "\n",
    "        r = requests.get(url, stream=True)\n",
    "        with open(zip_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extract(glove_file, glove_dir)\n",
    "        logger.info(\"GloVe extracted to %s\", glove_path)\n",
    "    else:\n",
    "        logger.warning(\"GloVe not found and download_if_missing=False. Will use random embeddings.\")\n",
    "\n",
    "    return glove_path\n",
    "\n",
    "# Define global GLOVE_PATH for later cells\n",
    "GLOVE_PATH = ensure_glove_100d()\n",
    "\n",
    "# ------------------------------------\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Builds TF-IDF matrix for classical models and tokenizer+sequences for deep models.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_num_words: int = config.max_num_words, max_seq_len: int = config.max_seq_length):\n",
    "        self.max_num_words = max_num_words\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tfidf_vectorizer: Optional[TfidfVectorizer] = None\n",
    "        self.tokenizer: Optional[Tokenizer] = None\n",
    "\n",
    "    def build_tfidf(self, texts: List[str]) -> Any:\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(max_features=self.max_num_words)\n",
    "        X_tfidf = self.tfidf_vectorizer.fit_transform(texts)\n",
    "        logger.info(\"Built TF-IDF: shape=%s\", X_tfidf.shape)\n",
    "        return X_tfidf\n",
    "\n",
    "    def build_tokenizer_and_sequences(self, texts: List[str]) -> np.ndarray:\n",
    "        self.tokenizer = Tokenizer(num_words=self.max_num_words, oov_token=None)\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "        seqs = self.tokenizer.texts_to_sequences(texts)\n",
    "        X_seq = pad_sequences(seqs, maxlen=self.max_seq_len)\n",
    "        logger.info(\"Built tokenizer: vocab_size=%d sequences shape=%s\", len(self.tokenizer.word_index), X_seq.shape)\n",
    "        return X_seq\n",
    "\n",
    "# ------------------------------------\n",
    "class EmbeddingManagerLocal:\n",
    "    \"\"\"\n",
    "    Build embedding matrix. Prefer GloVe if available; else fallback to random.\n",
    "    \"\"\"\n",
    "    def __init__(self, glove_path: str = GLOVE_PATH, max_num_words: int = config.max_num_words, emb_dim: int = config.embedding_dim):\n",
    "        self.glove_path = glove_path\n",
    "        self.max_num_words = max_num_words\n",
    "        self.emb_dim = emb_dim\n",
    "        self.glove_index: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    def _load_glove_index(self):\n",
    "        if self.glove_index:\n",
    "            return self.glove_index\n",
    "        if not os.path.exists(self.glove_path):\n",
    "            logger.warning(\"GloVe file not found at %s. Will use random embeddings.\", self.glove_path)\n",
    "            self.glove_index = {}\n",
    "            return self.glove_index\n",
    "\n",
    "        logger.info(\"Loading GloVe from %s (this may take a while)...\", self.glove_path)\n",
    "        idx = {}\n",
    "        with open(self.glove_path, 'r', encoding='utf8', errors='ignore') as fh:\n",
    "            for line in fh:\n",
    "                parts = line.rstrip().split(\" \")\n",
    "                word = parts[0]\n",
    "                vals = np.asarray(parts[1:], dtype=np.float32)\n",
    "                if vals.shape[0] == self.emb_dim:\n",
    "                    idx[word] = vals\n",
    "        self.glove_index = idx\n",
    "        logger.info(\"Loaded %d GloVe vectors.\", len(idx))\n",
    "        return idx\n",
    "\n",
    "    def build_embedding_matrix(self, word_index: Dict[str,int]) -> np.ndarray:\n",
    "        rng = np.random.RandomState(config.seed)\n",
    "        embedding_matrix = rng.normal(scale=0.01, size=(self.max_num_words, self.emb_dim)).astype(np.float32)\n",
    "\n",
    "        if os.path.exists(self.glove_path):\n",
    "            glove_index = self._load_glove_index()\n",
    "            found = 0\n",
    "            for word, idx in word_index.items():\n",
    "                if idx >= self.max_num_words:\n",
    "                    continue\n",
    "                vec = glove_index.get(word)\n",
    "                if vec is not None:\n",
    "                    embedding_matrix[idx] = vec\n",
    "                    found += 1\n",
    "            logger.info(\"Embedding matrix built: %d tokens found in GloVe (out of %d)\", found, min(len(word_index), self.max_num_words))\n",
    "        else:\n",
    "            logger.info(\"GloVe not used. Returning random-initialized embedding matrix of shape %s\", embedding_matrix.shape)\n",
    "        return embedding_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e2885c",
   "metadata": {},
   "source": [
    "Cell 7 — Classical models manager (GridSearchCV, fit, evaluate, feature importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc42ed03",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cell 7 — Classical models manager\n",
    "# ============================\n",
    "# Purpose:\n",
    "# - Provide GridSearchCV tuning for logistic and linear SVM\n",
    "# - Evaluate models with CV and final holdout\n",
    "# - Extract feature importance for logistic regression pipeline (top positive / negative tokens)\n",
    "\n",
    "from typing import List, Dict, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "class ClassicalPipelineManager:\n",
    "    \"\"\"\n",
    "    Manage classical ML pipelines (CountVectorizer -> TfidfTransformer -> classifier)\n",
    "    Supports grid search for hyperparameter tuning and returns summary DataFrame.\n",
    "    \"\"\"\n",
    "    def __init__(self, cv_folds: int = config.classical_cv_folds, n_jobs: int = config.grid_search_n_jobs):\n",
    "        self.cv_folds = cv_folds\n",
    "        self.n_jobs = n_jobs\n",
    "        # Base pipelines (we use TfidfTransformer after CountVectorizer)\n",
    "        self.base_pipelines = {\n",
    "            'naive_bayes': Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())]),\n",
    "            'logistic': Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', LogisticRegression(max_iter=2000))]),\n",
    "            'linear_svc': Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', LinearSVC(max_iter=5000))])\n",
    "        }\n",
    "        self.trained: Dict[str, Pipeline] = {}\n",
    "\n",
    "    def grid_search_and_cv(self, X: List[str], y: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform cross-validation and grid search where relevant.\n",
    "        Returns a DataFrame summarizing model performance (mean F1).\n",
    "        Trained estimators are stored in self.trained.\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        skf = StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=config.seed)\n",
    "\n",
    "        # Naive Bayes: no grid, but CV for baseline\n",
    "        logger.info(\"Running cross-val for Naive Bayes...\")\n",
    "        nb_scores = cross_val_score(self.base_pipelines['naive_bayes'], X, y, cv=skf, scoring='f1', n_jobs=self.n_jobs)\n",
    "        rows.append({'model': 'naive_bayes', 'f1_mean': float(nb_scores.mean()), 'f1_std': float(nb_scores.std())})\n",
    "        # Train NB on full data so it is available for inference later\n",
    "        self.base_pipelines['naive_bayes'].fit(X, y)\n",
    "        self.trained['naive_bayes'] = self.base_pipelines['naive_bayes']\n",
    "\n",
    "        # Logistic Regression: grid search over C\n",
    "        logger.info(\"Grid searching Logistic Regression...\")\n",
    "        param_grid_log = {'clf__C': [0.01, 0.1, 1, 10]}\n",
    "        grid_log = GridSearchCV(self.base_pipelines['logistic'], param_grid_log, cv=skf, scoring='f1', n_jobs=self.n_jobs)\n",
    "        grid_log.fit(X, y)\n",
    "        rows.append({'model': 'logistic', 'f1_mean': float(grid_log.best_score_), 'f1_std': float(np.std(grid_log.cv_results_['mean_test_score'])), 'best_params': grid_log.best_params_})\n",
    "        self.trained['logistic'] = grid_log.best_estimator_\n",
    "\n",
    "        # LinearSVC: grid search over C\n",
    "        logger.info(\"Grid searching LinearSVC...\")\n",
    "        param_grid_svc = {'clf__C': [0.01, 0.1, 1]}\n",
    "        grid_svc = GridSearchCV(self.base_pipelines['linear_svc'], param_grid_svc, cv=skf, scoring='f1', n_jobs=self.n_jobs)\n",
    "        grid_svc.fit(X, y)\n",
    "        rows.append({'model': 'linear_svc', 'f1_mean': float(grid_svc.best_score_), 'f1_std': float(np.std(grid_svc.cv_results_['mean_test_score'])), 'best_params': grid_svc.best_params_})\n",
    "        self.trained['linear_svc'] = grid_svc.best_estimator_\n",
    "\n",
    "        df_results = pd.DataFrame(rows).sort_values('f1_mean', ascending=False).reset_index(drop=True)\n",
    "        logger.info(\"Classical model tuning complete. Summary:\\n%s\", df_results.to_string(index=False))\n",
    "        return df_results\n",
    "\n",
    "    def evaluate(self, model_name: str, X_test: List[str], y_test: np.ndarray) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate a trained pipeline on test data and return metrics dictionary including classification report (dict).\n",
    "        \"\"\"\n",
    "        if model_name not in self.trained:\n",
    "            raise RuntimeError(f\"Model '{model_name}' is not trained.\")\n",
    "        pipe = self.trained[model_name]\n",
    "        preds = pipe.predict(X_test)\n",
    "        probs = None\n",
    "        # Try to obtain probabilities if available\n",
    "        try:\n",
    "            probs = pipe.predict_proba(X_test)[:, 1]\n",
    "        except Exception:\n",
    "            # Many classifiers (LinearSVC) do not implement predict_proba\n",
    "            probs = None\n",
    "        report = classification_report(y_test, preds, digits=4, output_dict=True)\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        logger.info(\"Evaluation complete for %s: accuracy=%.4f f1=%.4f\", model_name, acc, f1)\n",
    "        return {'accuracy': acc, 'f1': f1, 'report': report, 'preds': preds, 'probs': probs, 'pipeline': pipe}\n",
    "\n",
    "    def feature_importance_logistic(self, pipeline_obj: Pipeline, top_k: int = 20) -> Dict[str, List[tuple]]:\n",
    "        \"\"\"\n",
    "        Extract top positive/negative features from a trained logistic regression pipeline.\n",
    "        Expects pipeline to include 'vect' and 'clf' steps.\n",
    "        Returns dict: {'top_positive': [(token,coef),...], 'top_negative': [...]}\n",
    "        \"\"\"\n",
    "        vect = pipeline_obj.named_steps['vect']\n",
    "        clf = pipeline_obj.named_steps['clf']\n",
    "        if not hasattr(clf, 'coef_'):\n",
    "            raise RuntimeError(\"Classifier does not expose coefficients.\")\n",
    "        feature_names = vect.get_feature_names_out()\n",
    "        coefs = clf.coef_.flatten()\n",
    "        top_pos_idx = np.argsort(coefs)[-top_k:][::-1]\n",
    "        top_neg_idx = np.argsort(coefs)[:top_k]\n",
    "        top_pos = [(feature_names[i], float(coefs[i])) for i in top_pos_idx]\n",
    "        top_neg = [(feature_names[i], float(coefs[i])) for i in top_neg_idx]\n",
    "        logger.info(\"Extracted top %d positive and negative logistic features.\", top_k)\n",
    "        return {'top_positive': top_pos, 'top_negative': top_neg}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb721008",
   "metadata": {},
   "source": [
    "Cell 8 — Deep model: BiLSTM + Attention (build, train, save, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e39287",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cell 8 — Deep model: BiLSTM + Attention\n",
    "# ============================\n",
    "# Purpose:\n",
    "# - Build a BiLSTM model with attention mechanism\n",
    "# - Provide training utilities with early stopping, checkpointing and LR reduction\n",
    "# - Prediction helpers and saving/loading\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import timeit\n",
    "import numpy as np\n",
    "from typing import Optional, Dict\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Input, Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "class Attention(Layer):\n",
    "    \"\"\"\n",
    "    Lightweight attention layer that computes a learned weighted sum over time steps.\n",
    "    The returned context vector has shape (batch_size, features).\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # one trainable vector per feature dimension\n",
    "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1],), initializer='glorot_uniform', trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs: (batch, time, features)\n",
    "        e = tf.tensordot(inputs, self.W, axes=1)  # (batch, time)\n",
    "        e = tf.nn.tanh(e)\n",
    "        a = tf.nn.softmax(e, axis=1)              # (batch, time)\n",
    "        a = tf.expand_dims(a, axis=-1)            # (batch, time, 1)\n",
    "        context = tf.reduce_sum(inputs * a, axis=1)  # (batch, features)\n",
    "        return context\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "\n",
    "class BiLSTMAttn:\n",
    "    \"\"\"\n",
    "    BiLSTM followed by an attention layer and dense layers.\n",
    "    Methods:\n",
    "      - build(embedding_matrix=None, trainable_emb=False)\n",
    "      - train(X_train, y_train, X_val, y_val, ...)\n",
    "      - predict_proba(X)\n",
    "      - predict(X)\n",
    "      - save(path)\n",
    "      - load(path)\n",
    "    \"\"\"\n",
    "    def __init__(self, max_seq_len: int = config.max_seq_length, max_words: int = config.max_num_words, emb_dim: int = config.embedding_dim):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_words = max_words\n",
    "        self.emb_dim = emb_dim\n",
    "        self.model: Optional[Model] = None\n",
    "        self.checkpoint_path: Optional[str] = None\n",
    "\n",
    "    def build(self, embedding_matrix: Optional[np.ndarray] = None, trainable_emb: bool = False, dropout_rates=(0.25, 0.3, 0.5)):\n",
    "        \"\"\"\n",
    "        Build and compile the Keras model.\n",
    "        embedding_matrix: if provided, used to initialize the Embedding layer; otherwise random init.\n",
    "        trainable_emb: whether embedding layer is trainable (fine-tune).\n",
    "        dropout_rates: (recurrent_dropout, dropout_after_dense, final_dropout)\n",
    "        \"\"\"\n",
    "        inp = Input(shape=(self.max_seq_len,), name='input_ids')\n",
    "        if embedding_matrix is not None:\n",
    "            emb = Embedding(input_dim=self.max_words, output_dim=self.emb_dim, weights=[embedding_matrix],\n",
    "                            input_length=self.max_seq_len, mask_zero=True, trainable=trainable_emb, name='embedding')(inp)\n",
    "        else:\n",
    "            emb = Embedding(input_dim=self.max_words, output_dim=self.emb_dim, input_length=self.max_seq_len, mask_zero=True, name='embedding')(inp)\n",
    "\n",
    "        # BiLSTM block\n",
    "        x = Bidirectional(LSTM(128, return_sequences=True, dropout=dropout_rates[0]))(emb)\n",
    "\n",
    "        # Attention: compress sequence to context vector\n",
    "        context = Attention()(x)\n",
    "\n",
    "        # Dense head with dropout regularization\n",
    "        x = Dense(128, activation='relu')(context)\n",
    "        x = Dropout(dropout_rates[1])(x)\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "        x = Dropout(dropout_rates[2])(x)\n",
    "\n",
    "        out = Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "        self.model = Model(inputs=inp, outputs=out, name='bilstm_attention')\n",
    "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        logger.info(\"Built BiLSTM+Attention model with input shape %s and output shape %s\", self.model.input_shape, self.model.output_shape)\n",
    "        self.model.summary()\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val,\n",
    "          epochs: int = config.epochs, batch_size: int = config.batch_size, class_weights: Optional[Dict]=None):\n",
    "\n",
    "        \"\"\"\n",
    "        Train the model with early stopping, checkpointing and learning rate reduction.\n",
    "        Returns (history, duration_seconds).\n",
    "        \"\"\"\n",
    "        assert self.model is not None, \"Model not built: call build() first.\"\n",
    "        ckpt_path = os.path.join(config.save_dir, f\"bilstm_attn_best_{int(time.time())}.h5\")\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=config.patience, restore_best_weights=True, verbose=1),\n",
    "            ModelCheckpoint(ckpt_path, monitor='val_loss', save_best_only=True, verbose=1),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n",
    "        ]\n",
    "        start = timeit.default_timer()\n",
    "        history = self.model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size,\n",
    "                                 callbacks=callbacks, class_weight=class_weights, verbose=2)\n",
    "        duration = timeit.default_timer() - start\n",
    "        self.checkpoint_path = ckpt_path\n",
    "        logger.info(\"Training finished in %.2f seconds. Best weights saved to %s\", duration, ckpt_path)\n",
    "        return history, duration\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        assert self.model is not None, \"Model not built or loaded.\"\n",
    "        return self.model.predict(X).flatten()\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return (self.predict_proba(X) > 0.5).astype(int)\n",
    "\n",
    "    def save(self, path: str):\n",
    "        assert self.model is not None, \"No model to save.\"\n",
    "        self.model.save(path)\n",
    "        logger.info(\"Saved Keras model to %s\", path)\n",
    "\n",
    "    def load(self, path: str):\n",
    "        # load_model must be provided with custom_objects mapping for Attention\n",
    "        self.model = load_model(path, custom_objects={'Attention': Attention})\n",
    "        logger.info(\"Loaded Keras model from %s\", path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7214ae7",
   "metadata": {},
   "source": [
    "Cell 9 — Runner: orchestrate full experiment, persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbedf6c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cell 9 — Runner (Run Only)\n",
    "# ============================\n",
    "# Purpose:\n",
    "# - Execute all training, preprocessing, evaluation.\n",
    "# - Keep everything in memory (results, models, vectors).\n",
    "# - Saving/logging is moved to Cell 10.\n",
    "\n",
    "import hashlib\n",
    "import preprocessor as p\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import time, joblib, pickle, os, logging, warnings, timeit\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "class ExperimentRunner:\n",
    "    \"\"\"High-level orchestrator for experiments (training only).\"\"\"\n",
    "    def __init__(self, cfg: Config):\n",
    "        self.cfg = cfg\n",
    "        self.data_loader = DataLoaderLocal(cfg)\n",
    "        self.preproc = PreprocessorWithCache(\n",
    "            contractions=self.data_loader.load_contractions(),\n",
    "            cache_dir=cfg.cache_dir\n",
    "        )\n",
    "        self.feat = FeatureExtractor(\n",
    "            max_num_words=cfg.max_num_words,\n",
    "            max_seq_len=cfg.max_seq_length\n",
    "        )\n",
    "        self.emb_mgr = EmbeddingManagerLocal(\n",
    "            glove_path=GLOVE_PATH,\n",
    "            max_num_words=cfg.max_num_words,\n",
    "            emb_dim=cfg.embedding_dim\n",
    "        )\n",
    "        self.classical_mgr = ClassicalPipelineManager(\n",
    "            cv_folds=cfg.classical_cv_folds,\n",
    "            n_jobs=cfg.grid_search_n_jobs\n",
    "        )\n",
    "        self.deep_model_wrapper = BiLSTMAttn(\n",
    "            max_seq_len=cfg.max_seq_length,\n",
    "            max_words=cfg.max_num_words,\n",
    "            emb_dim=cfg.embedding_dim\n",
    "        )\n",
    "\n",
    "    def run(self, sentiment_sample=None, depressive_rows=None):\n",
    "        \"\"\"Execute full pipeline and return results in memory only.\"\"\"\n",
    "        sentiment_sample = sentiment_sample or self.cfg.sentiment_sample\n",
    "        depressive_rows = depressive_rows or self.cfg.depressive_rows\n",
    "\n",
    "        # 1. Load data\n",
    "        df_pos = self.data_loader.load_sentiment140(sample_n=sentiment_sample)\n",
    "        df_neg = self.data_loader.load_depressive(nrows=depressive_rows)\n",
    "        df = pd.concat([df_pos, df_neg], ignore_index=True).sample(\n",
    "            frac=1, random_state=self.cfg.seed\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        # 2. Clean text with cache\n",
    "        hash_key = hashlib.sha1(\"\".join(df['text'].fillna('').astype(str)).encode('utf8')).hexdigest()[:8]\n",
    "        df['clean'] = self.preproc.clean_texts(df['text'].fillna('').tolist(), cache_key=f\"cleaned_{hash_key}\")\n",
    "\n",
    "        # 3. Features\n",
    "        X_tfidf = self.feat.build_tfidf(df['clean'].tolist())\n",
    "        X_seq = self.feat.build_tokenizer_and_sequences(df['clean'].tolist())\n",
    "        y = df['label'].values\n",
    "\n",
    "        # 4. Classical\n",
    "        t0 = timeit.default_timer()\n",
    "        cv_df = self.classical_mgr.grid_search_and_cv(df['clean'].tolist(), y)\n",
    "        classical_seconds = timeit.default_timer() - t0\n",
    "        best_classical = cv_df.iloc[0]['model']\n",
    "        _, X_test_texts, _, y_test_texts = train_test_split(\n",
    "            df['clean'].tolist(), y, test_size=0.3, stratify=y, random_state=self.cfg.seed\n",
    "        )\n",
    "        classical_eval = self.classical_mgr.evaluate(best_classical, X_test_texts, y_test_texts)\n",
    "\n",
    "        # 5. Embedding matrix\n",
    "        embedding_matrix = self.emb_mgr.build_embedding_matrix(self.feat.tokenizer.word_index)\n",
    "\n",
    "        # 6. Deep model\n",
    "        self.deep_model_wrapper.build(embedding_matrix=embedding_matrix, trainable_emb=False)\n",
    "        Xtr, Xte, ytr, yte = train_test_split(\n",
    "            X_seq, y, test_size=0.3, stratify=y, random_state=self.cfg.seed\n",
    "        )\n",
    "        cw_vals = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(ytr), y=ytr)\n",
    "        class_weights = {int(c): float(w) for c, w in zip(np.unique(ytr), cw_vals)}\n",
    "        history, deep_seconds = self.deep_model_wrapper.train(\n",
    "            Xtr, ytr, Xte, yte,\n",
    "            epochs=self.cfg.deep_epochs,\n",
    "            batch_size=self.cfg.deep_batch_size,\n",
    "            class_weights=class_weights\n",
    "        )\n",
    "\n",
    "        # Deep eval\n",
    "        preds = (self.deep_model_wrapper.predict_proba(Xte) > 0.5).astype(int)\n",
    "        deep_eval = {\n",
    "            \"accuracy\": float(accuracy_score(yte, preds)),\n",
    "            \"f1\": float(f1_score(yte, preds)),\n",
    "            \"report\": classification_report(yte, preds, digits=4, output_dict=True)\n",
    "        }\n",
    "\n",
    "        # Feature importance (optional)\n",
    "        feature_importance = None\n",
    "        if \"logistic\" in self.classical_mgr.trained:\n",
    "            try:\n",
    "                feature_importance = self.classical_mgr.feature_importance_logistic(\n",
    "                    self.classical_mgr.trained[\"logistic\"], top_k=30\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.warning(\"Feature importance error: %s\", e)\n",
    "\n",
    "        # Store everything in memory only\n",
    "        self.df = df\n",
    "        results = {\n",
    "            \"cv\": cv_df,\n",
    "            \"classical_eval\": classical_eval,\n",
    "            \"deep_eval\": deep_eval,\n",
    "            \"feature_importance\": feature_importance,\n",
    "            \"history\": history,\n",
    "            \"durations\": {\"classical_seconds\": classical_seconds, \"deep_seconds\": deep_seconds},\n",
    "        }\n",
    "        logger.info(\"✅ Experiment complete (no saving yet).\")\n",
    "        return results\n",
    "\n",
    "\n",
    "# --- Run the experiment (train only, no saving) ---\n",
    "runner = ExperimentRunner(config)\n",
    "results = runner.run(config.sentiment_sample, config.depressive_rows)\n",
    "print(\"✅ Training complete. Proceed to Cell 10 to save artifacts & logs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abdffbb",
   "metadata": {},
   "source": [
    "Cell 10 — Save & Log Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca025520",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cell 10 — Save & Log Results\n",
    "# ============================\n",
    "# Purpose:\n",
    "# - Save all experiment artifacts (models, metrics, logs) after training.\n",
    "# - Avoid re-running heavy computations.\n",
    "# - Automatically store files in Google Drive or local folder.\n",
    "\n",
    "import os, json, joblib, pickle\n",
    "from datetime import datetime\n",
    "\n",
    "def save_experiment_artifacts(results, runner, cfg, save_to_drive=True):\n",
    "    \"\"\"Save trained models, results, and logs safely.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    save_dir = cfg.save_dir\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # --- Save Classical Models ---\n",
    "    artifacts = {}\n",
    "    for model_name, model_obj in runner.classical_mgr.trained.items():\n",
    "        path = os.path.join(save_dir, f\"{model_name}_model_{timestamp}.joblib\")\n",
    "        joblib.dump(model_obj, path)\n",
    "        artifacts[f\"{model_name}_model\"] = path\n",
    "\n",
    "    # --- Save Deep Model ---\n",
    "    deep_model_path = os.path.join(save_dir, f\"deep_model_{timestamp}.h5\")\n",
    "    runner.deep_model_wrapper.model.save(deep_model_path)\n",
    "    artifacts[\"deep_model\"] = deep_model_path\n",
    "\n",
    "    # --- Save Feature Extractor & Tokenizer ---\n",
    "    tok_path = os.path.join(save_dir, f\"tokenizer_{timestamp}.pkl\")\n",
    "    with open(tok_path, \"wb\") as f:\n",
    "        pickle.dump(runner.feat.tokenizer, f)\n",
    "    artifacts[\"tokenizer\"] = tok_path\n",
    "\n",
    "    # --- Save Metrics and History ---\n",
    "    results_path = os.path.join(save_dir, f\"results_{timestamp}.json\")\n",
    "    def safe(o):\n",
    "        if isinstance(o, (int, float, str, bool)) or o is None:\n",
    "            return o\n",
    "        if isinstance(o, (list, dict)):\n",
    "            return o\n",
    "        return str(o)\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(results, f, default=safe, indent=2)\n",
    "    artifacts[\"results_json\"] = results_path\n",
    "\n",
    "    # --- Save Experiment Summary ---\n",
    "    summary = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"deep_accuracy\": results[\"deep_eval\"][\"accuracy\"],\n",
    "        \"deep_f1\": results[\"deep_eval\"][\"f1\"],\n",
    "        \"best_classical_acc\": results[\"classical_eval\"][\"accuracy\"],\n",
    "        \"classical_time\": results[\"durations\"][\"classical_seconds\"],\n",
    "        \"deep_time\": results[\"durations\"][\"deep_seconds\"]\n",
    "    }\n",
    "    summary_path = os.path.join(save_dir, f\"summary_{timestamp}.txt\")\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        for k, v in summary.items():\n",
    "            f.write(f\"{k}: {v}\\n\")\n",
    "    artifacts[\"summary_txt\"] = summary_path\n",
    "\n",
    "    print(f\"✅ All artifacts saved in {save_dir}\")\n",
    "    return artifacts\n",
    "\n",
    "\n",
    "# --- Execute saving ---\n",
    "artifacts = save_experiment_artifacts(results, runner, config, save_to_drive=True)\n",
    "print(\"✅ Artifacts successfully saved and ready in Google Drive or local folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbcc284",
   "metadata": {},
   "source": [
    "Cell 11 — Visualization, Inference API, latency, requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d61befc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cell 11 — Visualization, Inference API, latency & requirements.txt\n",
    "# ============================\n",
    "# Purpose:\n",
    "# - Provide plotting utilities for results (confusion matrices, ROC/PR curves, training plots)\n",
    "# - Provide an inference service for serving predictions on new texts (both classical & deep)\n",
    "# - Provide simple requirements.txt writing for reproducibility\n",
    "\n",
    "# --- Visualization helpers ---\n",
    "def plot_confusion_normalized(y_true, y_pred, title=\"Confusion Matrix (normalized)\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap='Blues', xticklabels=['Non','Dep'], yticklabels=['Non','Dep'])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_pr(y_true, probs, label=\"Model\"):\n",
    "    # ROC\n",
    "    fpr, tpr, _ = roc_curve(y_true, probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC={roc_auc:.3f}\")\n",
    "    plt.plot([0,1],[0,1],'k--')\n",
    "    plt.title(f\"{label} ROC\")\n",
    "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.legend()\n",
    "    plt.show()\n",
    "    # Precision-Recall\n",
    "    prec, rec, _ = precision_recall_curve(y_true, probs)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(rec, prec)\n",
    "    plt.title(f\"{label} Precision-Recall\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "    plt.show()\n",
    "\n",
    "# --- Basic reporting using results from runner ---\n",
    "# Classical test predictions for display\n",
    "best_classical = results['classical_eval']['pipeline']\n",
    "# deep predictions already computed during runner.run -> deep_eval\n",
    "deep_eval = results['deep_eval']\n",
    "deep_report = deep_eval['report']\n",
    "\n",
    "print(\"\\n=== Final Summary ===\")\n",
    "print(\"Classical (best) evaluation:\")\n",
    "print(\" - Accuracy:\", results['classical_eval']['accuracy'])\n",
    "print(\" - F1:\", results['classical_eval']['f1'])\n",
    "print(\"\\nDeep model evaluation:\")\n",
    "print(\" - Accuracy:\", deep_eval['accuracy'])\n",
    "print(\" - F1:\", deep_eval['f1'])\n",
    "\n",
    "# If probability arrays exist, show ROC/PR for deep\n",
    "if 'probs' in deep_eval and deep_eval['probs'] is not None:\n",
    "    plot_roc_pr(yte_seq, deep_eval['probs'], label=\"Deep BiLSTM+Attention\")\n",
    "else:\n",
    "    # We have deep_probs local variable in the runner scope; attempt to use it\n",
    "    try:\n",
    "        plot_roc_pr(yte_seq, deep_probs, label=\"Deep BiLSTM+Attention\")\n",
    "    except Exception:\n",
    "        logger.info(\"Deep probability array not available for ROC plotting.\")\n",
    "\n",
    "# Confusion matrices\n",
    "try:\n",
    "    plot_confusion_normalized(yte_seq, deep_preds, title=\"Deep BiLSTM+Attn Confusion (normalized)\")\n",
    "except Exception:\n",
    "    logger.info(\"Deep confusion matrix plotting skipped (missing arrays).\")\n",
    "\n",
    "# If logistic feature importance exists, print small lists\n",
    "if results.get('feature_importance'):\n",
    "    print(\"\\nTop positive logistic features (indicative of depressive):\")\n",
    "    print(results['feature_importance']['top_positive'][:20])\n",
    "    print(\"\\nTop negative logistic features (indicative of non-depressive):\")\n",
    "    print(results['feature_importance']['top_negative'][:20])\n",
    "\n",
    "# --- Inference service for predictions on new texts ---\n",
    "class InferenceService:\n",
    "    \"\"\"\n",
    "    Lightweight inference wrapper combining preprocessor, tokenizer, classical pipeline, and deep Keras model.\n",
    "    Example:\n",
    "      inf = InferenceService(preprocessor, tokenizer, classical_pipeline, deep_model_wrapper)\n",
    "      inf.predict([\"some text\"])\n",
    "    \"\"\"\n",
    "    def __init__(self, preprocessor: PreprocessorWithCache, tokenizer: Tokenizer, classical_pipeline: Pipeline, deep_wrapper: BiLSTMAttn):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.classical_pipeline = classical_pipeline\n",
    "        self.deep_wrapper = deep_wrapper\n",
    "\n",
    "    def prepare(self, texts: List[str]):\n",
    "        cleaned = [self.preprocessor.clean_text(t) for t in texts]\n",
    "        seqs = self.tokenizer.texts_to_sequences(cleaned)\n",
    "        padded = pad_sequences(seqs, maxlen=config.max_seq_length)\n",
    "        return cleaned, padded\n",
    "\n",
    "    def predict(self, texts: List[str]):\n",
    "        cleaned, padded = self.prepare(texts)\n",
    "        # deep timings\n",
    "        t0 = timeit.default_timer()\n",
    "        deep_probs = self.deep_wrapper.predict_proba(padded)\n",
    "        t1 = timeit.default_timer()\n",
    "        avg_latency = (t1 - t0) / max(1, len(texts))\n",
    "        deep_labels = (deep_probs > 0.5).astype(int).tolist()\n",
    "\n",
    "        # classical predictions (try to get probabilities)\n",
    "        classical_probs = None\n",
    "        try:\n",
    "            classical_probs = self.classical_pipeline.predict_proba(cleaned)[:, 1]\n",
    "            classical_labels = self.classical_pipeline.predict(cleaned).tolist()\n",
    "        except Exception:\n",
    "            classical_labels = self.classical_pipeline.predict(cleaned).tolist()\n",
    "\n",
    "        results_list = []\n",
    "        for i, txt in enumerate(texts):\n",
    "            results_list.append({\n",
    "                'text': txt,\n",
    "                'cleaned': cleaned[i],\n",
    "                'deep_prob': float(deep_probs[i]),\n",
    "                'deep_label': int(deep_labels[i]),\n",
    "                'classical_prob': float(classical_probs[i]) if classical_probs is not None else None,\n",
    "                'classical_label': int(classical_labels[i]) if classical_labels is not None else None\n",
    "            })\n",
    "        return {'results': results_list, 'avg_latency_s': avg_latency}\n",
    "\n",
    "# Build inference service using in-memory artifacts\n",
    "inference_service = InferenceService(runner.preproc, runner.feat.tokenizer, best_classical, runner.deep_model_wrapper)\n",
    "\n",
    "# Demo inference (replace with your own sentences)\n",
    "examples = [\n",
    "    \"I feel hopeless and tired of everything.\",\n",
    "    \"I had a really wonderful day and I'm grateful.\"\n",
    "]\n",
    "inf_out = inference_service.predict(examples)\n",
    "print(\"Example predictions (deep_label 1 indicates depressive):\")\n",
    "for r in inf_out['results']:\n",
    "    print(r)\n",
    "\n",
    "# --- Create a small requirements.txt useful for reproducing the environment ---\n",
    "reqs = [\n",
    "    \"numpy\", \"pandas\", \"scikit-learn\", \"tensorflow\", \"nltk\", \"tqdm\",\n",
    "    \"tweet-preprocessor\", \"wordcloud\", \"joblib\"\n",
    "]\n",
    "req_path = os.path.join(config.save_dir, f\"requirements_{now_str()}.txt\")\n",
    "with open(req_path, \"w\") as fh:\n",
    "    fh.write(\"\\n\".join(reqs))\n",
    "logger.info(\"Wrote minimal requirements to %s\", req_path)\n",
    "\n",
    "logger.info(\"All done. Experiment finished successfully.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
